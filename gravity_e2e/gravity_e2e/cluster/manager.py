import asyncio
import logging
import sys

if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib  # fallback for older Python
import json
import shutil
import subprocess
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Optional, List, Tuple

from eth_account import Account
from eth_account.signers.local import LocalAccount

from .node import Node, NodeState, NodeRole

LOG = logging.getLogger(__name__)


@dataclass
class ValidatorJoinParams:
    """Parameters for joining a validator to the network."""

    private_key: str
    stake_pool: Optional[str]  # EVM address of the StakePool contract
    consensus_public_key: str
    validator_network_address: str
    fullnode_network_address: str
    stake_amount: str = "10001.0"
    moniker: Optional[str] = None
    lockup_duration: int = 2592000  # 30 days in seconds


@dataclass
class ValidatorSet:
    """Result of validator list: 3 lists of Nodes."""

    active: List[Node] = field(default_factory=list)
    pending_inactive: List[Node] = field(default_factory=list)
    pending_active: List[Node] = field(default_factory=list)


# Standard devnet keys (Anvil/Hardhat defaults)
KNOWN_DEV_KEYS = [
    "0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80",  # 0xf39F...
    "0x59c6995e998f97a5a0044966f0945389dc9e86dae88c7a8412f4603b6b78690d",  # 0x7099...
    "0x5de4111afa1a4b94908f83103eb1f1706367c2e68ca870fc3fb9a804cdab365a",  # 0x3C44...
]


class Cluster:
    """
    Unified entry point for interacting with a Gravity Cluster.
    Wraps infrastructure scripts (`cluster/`) and provides RPC access.
    """

    def __init__(self, config_path: Path):
        self.config_path = config_path.resolve()
        if not self.config_path.exists():
            raise FileNotFoundError(f"Cluster config not found: {self.config_path}")

        with open(self.config_path, "rb") as f:
            self.config = tomllib.load(f)

        # Determine paths
        # Assuming config_path is like .../cluster/cluster.toml
        self.cluster_root = self.config_path.parent
        self.base_dir = Path(self.config["cluster"]["base_dir"])
        self.gravity_cli_path = self.base_dir / "gravity_cli"

        self.nodes: Dict[str, Node] = self._discover_nodes()

        # Cluster control scripts
        self.start_script = self.cluster_root / "start.sh"
        self.stop_script = self.cluster_root / "stop.sh"

    @property
    def faucet(self) -> Optional[LocalAccount]:
        """Returns primary (first) faucet account."""
        f = self.faucets
        return f[0] if f else None

    def get_bench_accounts(self, limit: Optional[int] = None) -> List[Account]:
        """
        Load accounts from accounts.csv with optional limit.

        Args:
            limit: Maximum number of accounts to load. None for all.

        Returns:
            List of Account objects.
        """
        import os

        artifacts_dir = os.environ.get("GRAVITY_ARTIFACTS_DIR")
        if not artifacts_dir:
            raise RuntimeError(
                "GRAVITY_ARTIFACTS_DIR environment variable is not set. "
                "This is required to locate accounts.csv generated by gravity_bench."
            )
        accounts_csv = Path(artifacts_dir) / "accounts.csv"

        if not accounts_csv.exists():
            LOG.debug(f"accounts.csv not found at {accounts_csv}")
            return []

        accounts = []
        with open(accounts_csv) as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("Address"):  # Skip header
                    continue
                if limit and len(accounts) >= limit:
                    break
                parts = line.split(", ")
                if len(parts) == 2:
                    try:
                        accounts.append(Account.from_key(parts[1]))
                    except Exception as e:
                        LOG.warning(
                            f"Failed to parse account from line: {line[:50]}... : {e}"
                        )

        LOG.info(
            f"Loaded {len(accounts)} bench accounts from {accounts_csv}"
            + (f" (limited to {limit})" if limit else "")
        )
        return accounts

    def _ensure_evm_account(self, node: Node) -> LocalAccount:
        """
        Ensure the node has an EVM account assigned.

        If node.evm_account is None, assigns accounts from get_bench_accounts()
        to ALL VALIDATOR nodes in order of their discovery.

        Args:
            node: The node to ensure has an EVM account.

        Returns:
            The node's EVM account.

        Raises:
            RuntimeError: If no accounts are available or node is not VALIDATOR.
        """
        if node.evm_account is not None:
            return node.evm_account

        # Get all VALIDATOR nodes
        validator_nodes = [
            n for n in self.nodes.values() if n.role == NodeRole.VALIDATOR
        ]

        if not validator_nodes:
            raise RuntimeError("No VALIDATOR nodes found in cluster")

        # Load accounts from accounts.csv
        accounts = self.get_bench_accounts(limit=len(validator_nodes))

        if len(accounts) < len(validator_nodes):
            raise RuntimeError(
                f"Not enough accounts in accounts.csv: need {len(validator_nodes)} "
                f"for VALIDATOR nodes, but only {len(accounts)} available. "
                f"Run 'make faucet' to generate accounts."
            )

        # Assign accounts to ALL VALIDATOR nodes
        for i, validator_node in enumerate(validator_nodes):
            if validator_node.evm_account is None:
                validator_node.evm_account = accounts[i]
                LOG.debug(
                    f"Assigned EVM account {accounts[i].address} to node {validator_node.id}"
                )

        LOG.info(f"Assigned EVM accounts to {len(validator_nodes)} VALIDATOR nodes")

        # Return the account for the requested node
        if node.evm_account is None:
            raise RuntimeError(
                f"Node {node.id} is not a VALIDATOR node, cannot assign EVM account"
            )

        return node.evm_account

    @property
    def faucets(self) -> List[LocalAccount]:
        """
        Returns all faucet accounts as LocalAccount instances.
        Matches addresses from config with private keys from:
        1. KNOWN_DEV_KEYS (Built-in devnet keys)
        2. genesis.secrets.keys (Config)
        """
        genesis = self.config.get("genesis", {})
        faucet_config = genesis.get("faucet", [])

        # Normalize to list
        if isinstance(faucet_config, dict):
            faucets = [faucet_config]
        elif isinstance(faucet_config, list):
            faucets = faucet_config
        else:
            faucets = []

        if not faucets:
            return []

        # Gather potential private keys
        candidate_keys = set(KNOWN_DEV_KEYS)

        secrets = genesis.get("secrets", {})
        if secrets and "keys" in secrets:
            candidate_keys.update(secrets["keys"])

        # Build address -> LocalAccount mapping
        key_map: Dict[str, LocalAccount] = {}
        for k in candidate_keys:
            try:
                if not k.startswith("0x"):
                    k = "0x" + k
                account = Account.from_key(k)
                key_map[account.address.lower()] = account
            except Exception as e:
                LOG.warning(f"Failed to derive address from key {k[:6]}...: {e}")

        # Match faucet addresses to accounts
        accounts: List[LocalAccount] = []
        for f in faucets:
            addr = f.get("address")
            if addr:
                account = key_map.get(addr.lower())
                if account:
                    accounts.append(account)
                else:
                    LOG.warning(f"Faucet address {addr} has no matching private key")

        return accounts

    def _discover_nodes(self) -> Dict[str, Node]:
        nodes = {}
        for node_cfg in self.config.get("nodes", []):
            node_id = node_cfg["id"]
            rpc_port = node_cfg.get("rpc_port")
            role = NodeRole.from_str(node_cfg.get("role"))

            # Determine where this node lives on disk
            # Inherited from cluster deploy logic: base_dir / node_id
            # Unless explicitly overridden in config (common pattern in this project)
            node_data_dir = node_cfg.get("data_dir")
            http_port = node_cfg.get("https_port")
            p2p_port = node_cfg.get("p2p_port")
            vfn_port = node_cfg.get("vfn_port")
            if not node_data_dir:
                infra_path = self.base_dir / node_id
            else:
                infra_path = self.base_dir / node_id

            nodes[node_id] = Node(
                id=node_id,
                rpc_port=rpc_port,
                infra_path=infra_path,
                cluster_config_path=self.config_path,
                role=role,
                http_port=http_port,
                p2p_port=p2p_port,
                vfn_port=vfn_port,
            )
        return nodes

    async def _run_script(self, script: Path, args: List[str] = None) -> bool:
        if not script.exists():
            # If scripts don't exist, we assume it's a remote/unmanaged cluster
            # Log warning but don't fail, just return False
            LOG.warning(f"Script not found: {script} (cluster might be unmanaged)")
            return False

        cmd = ["bash", str(script)]
        if args:
            cmd.extend(args)

        # We pass CONFIG_FILE env var to scripts as they expect it or default to sibling cluster.toml
        env = {"CONFIG_FILE": str(self.config_path)}
        # Merge with current env
        import os

        full_env = os.environ.copy()
        full_env.update(env)

        LOG.info(f"Running cluster script: {script.name} {' '.join(args or [])}")
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                env=full_env,
                cwd=str(self.cluster_root),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            async def log_stream(stream, level):
                while True:
                    line = await stream.readline()
                    if not line:
                        break
                    decoded = line.decode().strip()
                    if decoded:
                        LOG.log(level, f"[{script.name}] {decoded}")

            # Run stream readers concurrently
            await asyncio.gather(
                log_stream(proc.stdout, logging.INFO),
                # Use WARNING for stderr to distinguish, or INFO if script is noisy
                log_stream(proc.stderr, logging.WARNING),
            )

            returncode = await proc.wait()

            if returncode != 0:
                LOG.error(f"{script.name} failed with exit code {returncode}")
                return False
            else:
                LOG.info(f"{script.name} success.")
                return True
        except Exception as e:
            LOG.error(f"Exception running {script.name}: {e}")
            return False

    async def start(self) -> bool:
        """Runs start.sh (starts all nodes)."""
        # start.sh takes --config argument
        return await self._run_script(
            self.start_script, ["--config", str(self.config_path)]
        )

    async def stop(self) -> bool:
        """Runs stop.sh (stops all nodes)."""
        return await self._run_script(
            self.stop_script, ["--config", str(self.config_path)]
        )

    def get_node(self, node_id: str) -> Optional[Node]:
        """Get a specific node handle."""
        return self.nodes.get(node_id)

    # ========== Declarative API ==========

    async def get_live_nodes(self) -> List[Node]:
        """Returns list of nodes currently in RUNNING state."""
        live = []
        for node in self.nodes.values():
            state, _ = await node.get_state()
            if state == NodeState.RUNNING:
                live.append(node)
        return live

    async def get_dead_nodes(self) -> List[Node]:
        """Returns list of nodes in STOPPED or STALE state."""
        dead = []
        for node in self.nodes.values():
            state, _ = await node.get_state()
            if state in (NodeState.STOPPED, NodeState.STALE, NodeState.UNKNOWN):
                dead.append(node)
        return dead

    async def get_node_status(self, node_id: str) -> Optional[NodeState]:
        """Get current state of a specific node."""
        node = self.nodes.get(node_id)
        if node:
            state, _ = await node.get_state()
            return state
        return None

    async def set_full_live(self, timeout: int = 60) -> bool:
        """
        Ensure ALL nodes are RUNNING. Blocks until converged or timeout.
        Returns True if all nodes are RUNNING.
        """
        import time

        deadline = time.time() + timeout

        while time.time() < deadline:
            # Check all nodes
            states = {}
            for node in self.nodes.values():
                state, _ = await node.get_state()
                states[node.id] = state

            LOG.info(
                f"Convergence check: {[(nid, s.name) for nid, s in states.items()]}"
            )

            tasks = []
            all_running = True

            for node in self.nodes.values():
                state = states[node.id]

                if state != NodeState.RUNNING:
                    all_running = False
                    if state == NodeState.STOPPED:
                        LOG.info(f"Starting stopped node {node.id}...")
                        tasks.append(node.start())
                    elif state == NodeState.STALE:
                        LOG.info(f"Cleaning up stale node {node.id}...")
                        # Restart stale node

                        async def restart_node(n):
                            await n.stop()
                            await n.start()

                        tasks.append(restart_node(node))
                    elif state == NodeState.UNKNOWN:
                        LOG.info(f"Starting unknown node {node.id}...")
                        tasks.append(node.start())

            if all_running:
                LOG.info("All nodes are RUNNING.")
                return True

            # Execute all start/restart tasks concurrently
            if tasks:
                await asyncio.gather(*tasks)

            await asyncio.sleep(2)

        LOG.error(f"Failed to converge to full_live within {timeout}s")
        return False

    async def set_all_stopped(self, timeout: int = 60) -> bool:
        """
        Ensure ALL nodes are STOPPED. Blocks until converged or timeout.
        """
        import time

        deadline = time.time() + timeout

        while time.time() < deadline:
            all_stopped = True

            for node in self.nodes.values():
                state, _ = await node.get_state()
                if state != NodeState.STOPPED:
                    all_stopped = False
                    LOG.info(f"Stopping node {node.id} (state={state.name})...")
                    await node.stop()

            if all_stopped:
                LOG.info("All nodes are STOPPED.")
                return True

            await asyncio.sleep(2)

        LOG.error(f"Failed to stop all nodes within {timeout}s")
        return False

    async def set_live_nodes(self, n: int, timeout: int = 60) -> bool:
        """
        Ensure exactly N nodes are RUNNING. Stops extra or starts missing.
        Blocks until converged or timeout.
        """
        import time

        if n > len(self.nodes):
            LOG.error(f"Requested {n} live nodes but only {len(self.nodes)} exist")
            return False

        deadline = time.time() + timeout

        while time.time() < deadline:
            current_states = {}
            for node_id, node in self.nodes.items():
                current_states[node_id], _ = await node.get_state()

            live = [nid for nid, s in current_states.items() if s == NodeState.RUNNING]
            live_count = len(live)

            if live_count == n:
                LOG.info(f"Converged: {n} nodes are RUNNING.")
                return True

            node_list = list(self.nodes.values())
            if live_count < n:
                # Need to start more nodes
                dead = [
                    node
                    for node in node_list
                    if current_states[node.id] != NodeState.RUNNING
                ]
                to_start = n - live_count
                for node in dead[:to_start]:
                    LOG.info(f"Starting node {node.id} to reach target {n}...")
                    await node.start()
            else:
                # Need to stop some nodes
                live_nodes = [
                    node
                    for node in node_list
                    if current_states[node.id] == NodeState.RUNNING
                ]
                to_stop = live_count - n
                for node in live_nodes[:to_stop]:
                    LOG.info(f"Stopping node {node.id} to reach target {n}...")
                    await node.stop()

            await asyncio.sleep(2)

        LOG.error(f"Failed to set {n} live nodes within {timeout}s")
        return False

    async def set_node(
        self, node_id: str, target_state: NodeState, timeout: int = 30
    ) -> bool:
        """
        Set a specific node to target state (RUNNING or STOPPED).
        Blocks until converged or timeout.
        """
        import time

        node = self.nodes.get(node_id)
        if not node:
            LOG.error(f"Node {node_id} not found")
            return False

        if target_state not in (NodeState.RUNNING, NodeState.STOPPED):
            LOG.error(f"Can only set node to RUNNING or STOPPED, got {target_state}")
            return False

        deadline = time.time() + timeout

        while time.time() < deadline:
            state, _ = await node.get_state()

            if state == target_state:
                LOG.info(f"Node {node_id} is now {target_state.name}")
                return True

            if target_state == NodeState.RUNNING:
                await node.start()
            else:
                await node.stop()

            await asyncio.sleep(2)

        LOG.error(f"Failed to set {node_id} to {target_state.name} within {timeout}s")
        return False

    async def check_block_increasing(
        self, node_id: Optional[str] = None, timeout: int = 30, delta: int = 1
    ) -> bool:
        """
        Check if block height is increasing.
        :param node_id: If specified, check only this node. If None, check ALL currently RUNNING nodes.
        :param timeout: Max time to wait for increase.
        :param delta: Minimum block increase expected.
        :return: True if all unchecked nodes made progress.
        """
        if node_id:
            node = self.get_node(node_id)
            if not node:
                LOG.error(f"Node {node_id} not found")
                return False
            return await node.wait_for_block_increase(timeout=timeout, delta=delta)
        else:
            # Check all live nodes
            live_nodes = await self.get_live_nodes()
            if not live_nodes:
                LOG.warning("No live nodes to check progress for.")
                return False

            LOG.info(
                f"Checking block progress for {len(live_nodes)} nodes: {[n.id for n in live_nodes]}"
            )

            results = await asyncio.gather(
                *[
                    n.wait_for_block_increase(timeout=timeout, delta=delta)
                    for n in live_nodes
                ]
            )

            success = all(results)
            if success:
                LOG.info("All live nodes made progress.")
            else:
                LOG.error("Some nodes failed to make progress.")
            return success

    # ========== Validator Management API ==========

    def _get_first_rpc_url(self) -> str:
        """Get RPC URL from the first node."""
        first_node = next(iter(self.nodes.values()))
        return first_node.url

    async def validator_list(self, timeout: int = 60) -> ValidatorSet:
        """
        Get the current validator set from the chain.
        Updates each matched node's stake_pool field based on consensus_public_key matching.

        Returns:
            ValidatorSet with active, pending_inactive, and pending_active node lists.
        """
        rpc_url = self._get_first_rpc_url()
        list_cmd = [
            str(self.gravity_cli_path),
            "validator",
            "list",
            "--rpc-url",
            rpc_url,
        ]

        LOG.info(f"Executing validator list command...")
        LOG.debug(f"Command: {' '.join(list_cmd)}")

        process = await asyncio.create_subprocess_exec(
            *list_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
        stdout_str = stdout.decode() if stdout else ""
        stderr_str = stderr.decode() if stderr else ""

        if process.returncode != 0:
            LOG.error(f"Failed to list validators: {stderr_str}")
            raise RuntimeError(f"Failed to list validators: {stderr_str}")

        # Parse JSON output
        validator_data = json.loads(stdout_str)
        LOG.debug(f"Validator list output: {json.dumps(validator_data, indent=2)}")

        # Build consensus_public_key -> Node mapping
        consensus_key_to_node: Dict[str, Node] = {}
        for node in self.nodes.values():
            try:
                consensus_key_to_node[node.consensus_public_key] = node
            except FileNotFoundError:
                # Node identity not available
                pass

        result = ValidatorSet()

        def process_validators(validator_list: List[dict], target_list: List[Node]):
            for v in validator_list:
                consensus_pubkey = v.get("consensus_pubkey", "")
                validator_addr = v.get("validator", "")  # stake_pool address
                # Clean up address format (remove 0x prefix if present for matching)
                if validator_addr.startswith("0x"):
                    stake_pool_addr = validator_addr
                else:
                    stake_pool_addr = f"0x{validator_addr}"

                # Match by consensus_public_key
                matched_node = consensus_key_to_node.get(consensus_pubkey)
                if matched_node:
                    # Update node's stake_pool
                    matched_node.stake_pool = stake_pool_addr
                    target_list.append(matched_node)
                    LOG.debug(
                        f"Matched validator {stake_pool_addr} to node {matched_node.id}"
                    )

        process_validators(validator_data.get("active_validators", []), result.active)
        process_validators(
            validator_data.get("pending_inactive", []), result.pending_inactive
        )
        process_validators(
            validator_data.get("pending_active", []), result.pending_active
        )

        LOG.info(
            f"Validator list: active={[n.id for n in result.active]}, "
            f"pending_inactive={[n.id for n in result.pending_inactive]}, "
            f"pending_active={[n.id for n in result.pending_active]}"
        )

        return result

    async def validator_join(
        self,
        node_id: str,
        stake_amount: str = "1.0",
        moniker: Optional[str] = None,
        timeout: int = 120,
    ):
        """
        Join a node to the validator set.

        Args:
            node_id: ID of the node to join.
            stake_amount: Amount to stake in ETH (for creating StakePool if needed).
            moniker: Display name for the validator.
            timeout: Command timeout in seconds.

        Raises:
            ValueError: If node role is not VALIDATOR or GENESIS.
            RuntimeError: If command fails or no EVM account available.
        """
        node = self.get_node(node_id)
        if not node:
            raise ValueError(f"Node {node_id} not found")

        # Validate node role
        if node.role != NodeRole.VALIDATOR:
            raise ValueError(
                f"Node {node_id} has role {node.role.value}, " f"expected VALIDATOR"
            )

        # Ensure node has EVM account
        evm_account = self._ensure_evm_account(node)
        private_key = evm_account.key.hex()

        # Ensure stake_pool is populated
        if node.stake_pool is None:
            LOG.info(
                f"Node {node_id} has no stake_pool, fetching via validator_list..."
            )
            await self.validator_list()

        rpc_url = self._get_first_rpc_url()

        # Build join command
        join_cmd = [
            str(self.gravity_cli_path),
            "validator",
            "join",
            "--rpc-url",
            rpc_url,
            "--private-key",
            private_key,
            "--stake-amount",
            stake_amount,
            "--consensus-public-key",
            node.consensus_public_key,
            "--validator-network-address",
            f"/ip4/127.0.0.1/tcp/{node.p2p_port}/noise-ik/{node.identity.network_public_key}/handshake/0",
            "--fullnode-network-address",
            f"/ip4/127.0.0.1/tcp/{node.vfn_port}/noise-ik/{node.identity.network_public_key}/handshake/0",
        ]

        # Add stake_pool if available (for existing validators)
        if node.stake_pool:
            join_cmd.extend(["--stake-pool", node.stake_pool])

        # Add moniker
        effective_moniker = moniker or node_id.upper()
        join_cmd.extend(["--moniker", effective_moniker])

        LOG.info(f"Executing validator join for {node_id}...")
        LOG.debug(f"Command: {' '.join(join_cmd)}")

        process = await asyncio.create_subprocess_exec(
            *join_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            start_new_session=True,
        )

        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
        stdout_str = stdout.decode() if stdout else ""
        stderr_str = stderr.decode() if stderr else ""

        if process.returncode != 0:
            LOG.error(f"Failed to join validator {node_id}: {stderr_str}")
            raise RuntimeError(f"Failed to join validator {node_id}: {stderr_str}")

        LOG.info(f"Validator {node_id} join command executed successfully")
        if stdout_str:
            LOG.debug(f"Command output: {stdout_str}")

    async def validator_leave(self, node_id: str, timeout: int = 120):
        """
        Remove a node from the validator set.

        Args:
            node_id: ID of the node to leave.
            timeout: Command timeout in seconds.

        Raises:
            ValueError: If node role is not VALIDATOR or GENESIS, or stake_pool is unknown.
            RuntimeError: If command fails or no EVM account available.
        """
        node = self.get_node(node_id)
        if not node:
            raise ValueError(f"Node {node_id} not found")

        # Validate node role
        if node.role != NodeRole.VALIDATOR:
            raise ValueError(
                f"Node {node_id} has role {node.role.value}, " f"expected VALIDATOR"
            )

        # Ensure node has EVM account
        evm_account = self._ensure_evm_account(node)
        private_key = evm_account.key.hex()

        # Ensure stake_pool is populated
        if node.stake_pool is None:
            LOG.info(
                f"Node {node_id} has no stake_pool, fetching via validator_list..."
            )
            await self.validator_list()

        if node.stake_pool is None:
            raise ValueError(
                f"Node {node_id} stake_pool is still None after validator_list. "
                f"The node may not be registered as a validator."
            )

        rpc_url = self._get_first_rpc_url()

        leave_cmd = [
            str(self.gravity_cli_path),
            "validator",
            "leave",
            "--rpc-url",
            rpc_url,
            "--private-key",
            private_key,
            "--stake-pool",
            node.stake_pool,
        ]

        LOG.info(f"Executing validator leave for {node_id}...")
        LOG.debug(f"Command: {' '.join(leave_cmd)}")

        process = await asyncio.create_subprocess_exec(
            *leave_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            start_new_session=True,
        )

        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
        stdout_str = stdout.decode() if stdout else ""
        stderr_str = stderr.decode() if stderr else ""

        if process.returncode != 0:
            LOG.error(f"Failed to leave validator {node_id}: {stderr_str}")
            raise RuntimeError(f"Failed to leave validator {node_id}: {stderr_str}")

        LOG.info(f"Validator {node_id} leave command executed successfully")
        if stdout_str:
            LOG.debug(f"Command output: {stdout_str}")
